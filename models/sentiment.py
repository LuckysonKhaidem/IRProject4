# -*- coding: utf-8 -*-
"""Sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-kkdEuInSjCzS7Y9uXxvxsvV0XqJTeLV
"""
import re
import matplotlib
matplotlib.use("Agg")
from textblob import TextBlob
import json
#from google.colab import files
import matplotlib.pyplot as plt
#pip install googletrans
from googletrans import Translator
from pprint import pprint
import seaborn as sns
import pandas as pd
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
from nltk.probability import FreqDist
import mpld3
from mpld3 import plugins
import gensim
import uuid
import requests



class SentimentAnalyzer:
    
    def __init__(self,data):
        self.data=data
    
    def analyse_sentiment(self,tweets):
      for doc in tweets:
        analysis=TextBlob(doc['text_en'])
        if analysis.sentiment.polarity > 0:
            doc['sentiment']="Positive"
        elif analysis.sentiment.polarity == 0:
            doc['sentiment']="Neutral"
        else:
            doc['sentiment']="Negative"
      return tweets
  
    def translate_data(self,tweets):
        subscription_key = "02b7040aea8449f78447645b79c4c88b"
        endpoint = "https://api.cognitive.microsofttranslator.com/"
        path = '/translate?api-version=3.0'
        params = '&to=en'
        constructed_url = endpoint + path + params
        headers = {
            'Ocp-Apim-Subscription-Key': subscription_key,
            'Content-type': 'application/json',
            'X-ClientTraceId': str(uuid.uuid4())
        }
        for i in tweets:
          if("text_hi" in i  and i['text_hi']!=None):
            body=[{"text":i['text_hi']}]
            request = requests.post(constructed_url, headers=headers, json=body)
            response = request.json()
            i['text_en']=response[0]['translations'][0]['text']
          if("text_pt" in i and i['text_pt']!=None):
            body=[{"text":i['text_pt']}]
            request = requests.post(constructed_url, headers=headers, json=body)
            response = request.json()
            print(response)
            i['text_en']=response[0]['translations'][0]['text']
        return tweets
    
    def remove_stopwords(self,tweets):
        stop_words_en = stopwords.words('english')
        stop_words_en.extend(['RT','rt','@','would','could','should','said','must'])
        return [[word for word in doc if word not in stop_words_en] for doc in tweets]
    
    def tokenize_tweets(self,tweets):
        for doc in tweets:
            doc["text_en"] = re.sub(r"http\S+", "", doc["text_en"])
        tokenizer = RegexpTokenizer(r'\w+')
        tokenized_list_en = [tokenizer.tokenize(doc['text_en'].lower()) for doc in tweets]
        return tokenized_list_en
    
    def get_top10_words(self,tweets):
        word_list=[]
        for wrd_list in tweets:
          for wrd in wrd_list:
            word_list.append(wrd)
        fdist = FreqDist(word_list)
        """fd = pd.DataFrame(fdist.most_common(10),columns = ["Word","Frequency"]).drop([0]).reindex()
        fd_json=fd.to_dict()
        #print(fd_json)"""
        return fdist.most_common(10)

    def generate_top10_word_plot(self,topwrds):
        fig, ax = plt.subplots()
        freq_plot=ax.bar(topwrds['Word'],topwrds['Frequency'])
        ax.set_title('Top 10 Frequent Word in Replies',size=20)
        plt.xticks(topwrds['Word'], rotation='vertical')
        #fig.suptitle('Sentiment Analysis of Tweet Replies')
        tooltip = plugins.PointHTMLTooltip(freq_plot[0],voffset=10, hoffset=10)
        plugins.connect(fig, tooltip)
        return fig
    
    #generate json of sentiment
    def generate_sentiment_plot(self,data_with_sentiment):
        senti=[i['sentiment'] for i in data_with_sentiment]
        senti_df=pd.DataFrame(senti,columns=["Sentiment"])
        sums=senti_df['Sentiment'].count()
        senti_counts=senti_df["Sentiment"].value_counts()/sums
        #print(senti_counts)
        senti_counts_json=senti_counts.to_dict()
        """fig, ax = plt.subplots()
        senti_plot=ax.bar(senti_counts.index, senti_counts.values)
        ax.set_xlabel('Sentiment',size=12)
        #ax.set_ylabel('%age of Tweet Replies',size=12)
        ax.set_title('Sentiment Analysis of Tweet Replies', size=17)
        plt.xticks(senti_counts.index)
        
        tooltip = plugins.PointHTMLTooltip(senti_plot[0],voffset=10, hoffset=10)
        plugins.connect(fig, tooltip)
        return fig"""
        return senti_counts_json
    
    #call this method to get sentiment count in json format
    def get_sentiment_plot(self):
        translated_tweets=self.translate_data(self.data)
        tweets_with_sentments=self.analyse_sentiment(translated_tweets)
        senti_json=self.generate_sentiment_plot(tweets_with_sentments)
        #mygraph="/Users/ankitanand/Box/UB/Fall 2019/IR/Proj1/cooked/graph.html"
        #senti_graph_html=mpld3.save_html(senti_graph,mygraph)
        return senti_json
    
    #call this method to get top 10 words tuple list
    def get_top10_word_plot(self):
         translated_tweets=self.translate_data(self.data)
         tweets_with_sentments=self.analyse_sentiment(translated_tweets)
         tokenized_tweets=self.tokenize_tweets(tweets_with_sentments)
         without_stop_words=self.remove_stopwords(tokenized_tweets)
         wrd_list=self.get_top10_words(without_stop_words)
         print(wrd_list)
         #top10_wrds_graph=self.generate_top10_word_plot(wrd_list)
         #top10_fig_html=mpld3.fig_to_html(top10_wrds_graph)
         return wrd_list

"""data_path="/Users/ankitanand/Box/UB/Fall 2019/IR/Proj1/cooked/"
my_docs = open(data_path+'cooked_usa_18.json')
data = json.load(my_docs)
senti=SentimentAnalyzer(data)
senti_plt=senti.get_sentiment_plot() #use this method
freq_plt=senti.get_top10_word_plot()"""

