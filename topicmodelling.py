# -*- coding: utf-8 -*-
"""TopicModelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SJJX9UQYUwJTy8lp4Nxx7PMPtWlewpzi
"""

import gensim
import uuid
import requests
import os
import json
from gensim import corpora
from gensim.utils import simple_preprocess
from nltk.corpus import stopwords
import nltk
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import RegexpTokenizer
nltk.download('wordnet')
from gensim.models import LdaModel, LdaMulticore
import matplotlib.pyplot as plt
import pyLDAvis
import pyLDAvis.gensim
import re

class TopicModeller:
    def __init__(self,data):
        self.data=data
    
    def translate_data(self,tweets):
        subscription_key = "02b7040aea8449f78447645b79c4c88b"
        endpoint = "https://api.cognitive.microsofttranslator.com/"
        path = '/translate?api-version=3.0'
        params = '&to=en'
        constructed_url = endpoint + path + params
        headers = {
            'Ocp-Apim-Subscription-Key': subscription_key,
            'Content-type': 'application/json',
            'X-ClientTraceId': str(uuid.uuid4())
        }
        for i in tweets:
          if("text_hi" in i and i['text_hi']!=None):
            body=[{"text":i['text_hi']}]
            request = requests.post(constructed_url, headers=headers, json=body)
            response = request.json()
            i['text_en']=response[0]['translations'][0]['text']
          if("text_pt" in i and i['text_pt']!=None):
            body=[{"text":i['text_pt']}]
            request = requests.post(constructed_url, headers=headers, json=body)
            response = request.json()
            i['text_en']=response[0]['translations'][0]['text']
        return tweets
    
    def tokenize_tweets(self,tweets):
        for doc in tweets:
            doc["text_en"] = re.sub(r"http\S+", "", doc["text_en"])
        tokenizer = RegexpTokenizer(r'\w+')
        """for doc in tweets:
            doc['text_en']=doc['text_en'].lower()"""
        tokenized_list_en = [tokenizer.tokenize(doc['text_en'].lower()) for doc in tweets]
        return tokenized_list_en
    
    def remove_small_penis(self,tweets):
        return [[word for word in doc if len(word)>3] for doc in tweets]
    
    def remove_short_guys(self,tweets):
        return [doc for doc in tweets if len(doc)>20]
    
    def remove_digit(self,tweets):
        return [[word for word in doc if not word.isnumeric()] for doc in tweets]
    
    def remove_stopwords(self,tweets):
        stop_words_en = stopwords.words('english')
        stop_words_en.extend(['RT','rt','@','would','could','should','said','must'])
        return [[word for word in doc if word not in stop_words_en] for doc in tweets]
    
    def lemmatizer(self,tweets):
        wordnet_lemmatizer = WordNetLemmatizer()
        return [[wordnet_lemmatizer.lemmatize(word) for word in doc] for doc in tweets]
    
    def do_lda(self,tweets):
        dct = corpora.Dictionary(tweets)
        corpus = [dct.doc2bow(line) for line in tweets]
        lda_model = LdaMulticore(corpus=corpus,
                                 id2word=dct,
                                 random_state=100,
                                 num_topics=4,
                                 chunksize=100,
                                 passes=10,
                                 alpha='asymmetric',
                                 per_word_topics=True)
        return lda_model,dct,corpus
    
    def visualise_lda(self,lda_model,corpus,dct):
        vis = pyLDAvis.gensim.prepare(lda_model, corpus, dct)
        lda_html=pyLDAvis.prepared_data_to_html(vis)
        #data_path="/Users/ankitanand/Box/UB/Fall 2019/IR/Proj1/cooked/lda.html"
        #lda_html=pyLDAvis.save_html(vis,data_path)
        return lda_html
    
    def lda_graph(self):
        translated_tweets=self.translate_data(self.data)
        tokenised_tweets=self.tokenize_tweets(translated_tweets)
        tokenised_tweets=self.remove_short_guys(tokenised_tweets)
        tweets_cleaned=self.remove_stopwords(tokenised_tweets)
        tweets_cleaned=self.remove_small_penis(tweets_cleaned)
        tweets_cleaned=self.remove_digit(tweets_cleaned)
        tweets_lemmatised=self.lemmatizer(tweets_cleaned)
        try:
            lda_tweets,dct,corpus=self.do_lda(tweets_lemmatised)
            lda_graph=self.visualise_lda(lda_tweets,corpus,dct)
            return lda_graph
        except:
            return None
    
"""data_path="/Users/ankitanand/Box/UB/Fall 2019/IR/Proj1/test_lda/"
data=[]
for filename in os.listdir(data_path):
    with open(data_path + filename,'r') as f:
        #print(filename)
        tmp=json.load(f)
        data.extend(tmp)
#print(data)
senti=TopicModeller(data)
senti_plt=senti.lda_graph()"""
