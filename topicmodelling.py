# -*- coding: utf-8 -*-
"""TopicModelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SJJX9UQYUwJTy8lp4Nxx7PMPtWlewpzi
"""

import gensim
import uuid
import requests
import os
import json
from gensim import corpora
from gensim.utils import simple_preprocess
from nltk.corpus import stopwords
import nltk
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import RegexpTokenizer
nltk.download('wordnet')
from gensim.models import LdaModel, LdaMulticore
import matplotlib.pyplot as plt
import pyLDAvis
import pyLDAvis.gensim

class TopicModeller:
    def __init__(self,data):
        self.data=data
    
    def translate_data(self,tweets):
        subscription_key = "02b7040aea8449f78447645b79c4c88b"
        endpoint = "https://api.cognitive.microsofttranslator.com/"
        path = '/translate?api-version=3.0'
        params = '&to=en'
        constructed_url = endpoint + path + params
        headers = {
            'Ocp-Apim-Subscription-Key': subscription_key,
            'Content-type': 'application/json',
            'X-ClientTraceId': str(uuid.uuid4())
        }
        for i in tweets:
          if("text_hi" in i and i['text_hi']!=None):
            body=[{"text":i['text_hi']}]
            request = requests.post(constructed_url, headers=headers, json=body)
            response = request.json()
            i['text_en']=response[0]['translations'][0]['text']
          if("text_pt" in i and i['text_pt']!=None):
            body=[{"text":i['text_pt']}]
            request = requests.post(constructed_url, headers=headers, json=body)
            response = request.json()
            i['text_en']=response[0]['translations'][0]['text']
        return tweets
    
    def tokenize_tweets(self,tweets):
        tokenizer = RegexpTokenizer(r'\w+')
        tokenized_list_en = [tokenizer.tokenize(doc['text_en']) for doc in tweets]
        return tokenized_list_en
    
    def remove_stopwords(self,tweets):
        stop_words_en = stopwords.words('english')
        return [[word for word in doc if word not in stop_words_en] for doc in tweets]
    
    def lemmatizer(self,tweets):
        wordnet_lemmatizer = WordNetLemmatizer()
        return [[wordnet_lemmatizer.lemmatize(word) for word in doc] for doc in tweets]
    
    def do_lda(self,tweets):
        dct = corpora.Dictionary(tweets)
        corpus = [dct.doc2bow(line) for line in tweets]
        lda_model = LdaMulticore(corpus=corpus,
                                 id2word=dct,
                                 random_state=100,
                                 num_topics=4,
                                 passes=10,
                                 chunksize=1000,
                                 batch=False,
                                 alpha='asymmetric',
                                 decay=0.5,
                                 offset=64,
                                 eta=None,
                                 eval_every=0,
                                 iterations=100,
                                 gamma_threshold=0.001,
                                 per_word_topics=True)
        return lda_model,dct,corpus
    
    def visualise_lda(self,lda_model,corpus,dct):
        vis = pyLDAvis.gensim.prepare(lda_model, corpus, dct)
        lda_html=pyLDAvis.prepared_data_to_html(vis)
        #data_path="/Users/ankitanand/Box/UB/Fall 2019/IR/Proj1/cooked/lda.html"
        lda_html=pyLDAvis.prepared_data_to_html(vis)
        return lda_html
    
    def lda_graph(self):
        translated_tweets=self.translate_data(self.data)
        tokenised_tweets=self.tokenize_tweets(translated_tweets)
        tweets_cleaned=self.remove_stopwords(tokenised_tweets)
        tweets_lemmatised=self.lemmatizer(tweets_cleaned)
        lda_tweets,dct,corpus=self.do_lda(tweets_lemmatised)
        lda_graph=self.visualise_lda(lda_tweets,corpus,dct)
        return lda_graph
    
"""data_path="/Users/ankitanand/Box/UB/Fall 2019/IR/Proj1/cooked/"
my_docs = open(data_path+'cooked_india_18.json')
data = json.load(my_docs)
senti=TopicModeller(data)
senti_plt=senti.lda_graph()"""